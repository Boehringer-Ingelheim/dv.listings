---
title: "Review feature design document"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Review feature design document}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## User Requirements

These requirements are based on initial brainstorming conversations and a few rounds of user feedback. The list is not exhaustive:

- User can self-select a reviewer role and, under that capacity, annotate each row of any given dataset with a value chosen from those available on a dropdown menu.
- Several users can interact under _strictly non-overlapping_ roles with the application, each annotating individual rows of _possibly overlapping_ datasets.
- All decisions are stored. Even those that conflict (because they differ across roles). The last decision is the one visible by default.
- Rows are identified by a subset of the dataset's columns (which we call `identifier` columns). That subset is dataset-specific. No two rows can share the same set of identifier values.
- A subset of the columns (which we call `tracked` and does not overlap the `identifier` columns) is considered necessary and sufficient for review purposes.
- Updates to the provided datasets are expected during the course of a study. 
- Changes to contents of `tracked` columns of a previously reviewed dataset row will be highlighted in the user interface and require re-confirmation.
- All datasets share the same decision dropdown choices.

## API
This feature can be implemented by adding an extra parameter to `mod_listings`:
```
review = list(
  datasets = list(
    dm = list(
      id_vars = "USUBJID",
      tracked_vars = c("RFSTDTC", "RFENDTC", "RFXSTDTC", "TFXENDTC", "DMDTC")
    ),
    ae = list(
      id_vars = c("USUBJID", "AESEQ"),
      tracked_vars = c("AETERM", "AESTDTC", "AEENDTC", "AEOUT", "AEACN", "AEREL", "AESEV")
    ),
    ...
  ),
  choices = c("Seen", "Should look into", "Looked into", "We don't seem to agree", "Clearly artifactual"),
  roles = c("TSTAT", "SP", "Safety", "CTL"),
  store_path = '/mnt/path/to/app/storage'
)
```
All leafs are of type `character(n)`, except for `review_store_path` which is optional and of type character(1). 

A possible simplification would be to make `"USUBJID"` optional on `id_vars`, since `subjid_var` is already a mandatory parameter to the module. The logic inside the module would be that `sorted(union(subjid_var, id_vars))` would be use to identify each row.

**Beware**: Once the application is configured and run once, the only change permitted to the `datasets` subfield will be to *add* extra datasets. Changes to previously configured `id_vars` or `tracked_vars` sub-subfields could potentially render the collected review information inconsistent. The module should disallow the editing controls until such a situation is addressed. Review choices and roles do not suffer from that problem.

## Data Stability Requirements
The module only has access to the latest version of any given dataset. In order to inform users about modified and newly added records, it relies on stored summary hashes of previously seen data. Thus, it is necessary that some aspects of the representation of data are kept constant over the life of a study. Currently, these are:

- All rows of each provided dataset are identified uniquely by the combination of `id_vars` configured at the beginning of the study.
- Identifying variables (defined through `id_vars`) and tracked variables (defined through `tracked_vars`) have to remain the same for the duration of the study.
- Identifying and tracked variables retain their types (factor, numeric, ...) and are available on each revision of each dataset.
- No data rows are dropped during the study. In other words, if a combination of `id_vars` is present on revision `n` of a dataset, it will be available on revision `n+1`.

## User Interface
Basic features:

- Isolated drop-down to choose reviewer role. Blank every time the application starts. Not bookmarked. Only when a non-empty role is selected can the user review data.
- A listing set up for review will have three extra columns: 
  - Latest review decision
  - Latest reviewer role
  - Row status: unreviewed data, reviewed data, data modified after review, conflict across reviewers.

Sorting/Filtering by "row status" should allow to conduct reviews of incremental changes to the underlying dataset.

Future features (not requested, so not planned for this development phase):
- User upload/download of review information. For manual backup purposes. Stored data consists mostly of hashes, so plaintext download should be OK. However, if necessary we could encrypt it using a symmetric key configured as an app secret and provided as an extra parameter to the module.
- Load content from concurrent review sessions.
- Acceleration options (Bulk editting, keyboard controls, etc.) outside of initial implementation.
- Free text entry for each row and reviewer.

#### Open questions
- The module allows to tweak column visibility. Is it OK to allow review actions performed while some `tracked_vars` are not visible?

## Server storage

_None of the proposals of this section are in scope for the first version of the review functionality. Only the alternative "Client storage" explain in the next section is implemented_.

Currently, the two available forms of storage on Connect are:

- Pins
- Application bookmark folder

The nature of this task requires concurrent multi-user (multi-session, multi-process) write access to a shared resource, which Posit deems incompatible with pins (https://docs.posit.co/connect/user/pins/, see redish box).

This is the first DaVinci module that will serialize to permanent storage a substantial amount of study decisions taken over an extended period of time. Loss of data in this context is not acceptable. To guarantee data integrity, we propose the use of simple *append-only* data structures. Put simply, they are plain files opened in "append" mode. It's easy to demonstrate that they can't become the source of catastrophic loss of data of past review sessions. The granularity of the `pins` API (which only allows to load and store complete files) is incompatible with this design. 

Use of data appending is also beneficial for performance, as only new changes are serialized. It's also useful for version tracking, as all editing operations can be replayed in the order in which they happened.

Backing up review data can be accomplished by copying the contents of the container folder to an independent file system. Since all review operations are appendend, a delta-transmission tool such as `rsync` is a natural fit.

Lastly, using plain file storage allows for easy local testing and for universal use of the review feature (running the application locally, on Connect, Openshift, ...).

The optional `review_store_path` parameter allows to point to an arbitrary folder that should be available for reading and writing. If left unspecified, the module resorts to using the application bookmark folder.

**Note:** If we do indeed end up using bookmarking space for storage, we should check that the application is set up to use server bookmarks and not URL bookmarks.

#### Open questions
- What are the properties of the Connect bookmark folders? Are they backed up? Do we have a storage limit?
- Will client-controlled mount points become available at some point on Connect?

## Client Storage
An alternative approach to review data storage is to use Google's [File System Access API](https://wicg.github.io/file-system-access/) that is currently available in Chrome-derived browsers. To use it, reviewers have to point the app to a folder shared by the team at the beginning of each session.

## Data structures
There will use a small collection of files for each input dataset configured for review. If we take an imaginary "ae" domain, we would store the following files:

- `ae_000.base` (first known "ae" dataset):
  - 1 file magic code ("LISTBASE")
  - 1 format version number (0)
  - 1 generation marker (0)
  - 1 timestamp of creation date+time (UTC)
  - 1 complete hash of "ae" data.frame
  - 1 domain string ("ae")
  - n `id_vars` column names
  - n `id_vars` column types (see "Variable type encoding" below)
  - m `tracked_vars` column names
  - m `tracked_vars` column types (see "Variable type encoding" below)
  - 1 row count
  - p (1 per "ae" row) `hash_id(ae[id_vars])`
  - p (1 per "ae" row, *m* bytes long) `hash_tracked(ae[tracked_vars])`
- `ae_001.delta` (one per domain update):
  - 1 file magic code ("LISTDELT")
  - 1 format version number (0)
  - 1 generation marker (1)
  - 1 timestamp delta (seconds elapsed since "base" timestamp)
  - 1 complete hash of new "ae" data.frame
  - 1 domain string ("ae")
  - 1 count of new rows
  - n (1 per *new* "ae" row) `hash_id(ae[id_vars])`
  - n (1 per *new* "ae" row, *m* bytes long) `hash_tracked(ae[tracked_vars])`
  - 1 count of modified rows
  - p (1 per *modified* "ae" row) row index
  - p (1 per *modified* "ae" row, *m* bytes long) `hash_tracked(ae[tracked_vars])`
- `review.codes` (one):
  - 1 file magic code ("LISTCODE")
  - 1 format version number (0)
  - n review texts
- `ae_<ROLE>.review` (one per ROLE):
  - 1 file magic code ("LISTREVI")
  - 1 format version number (0)
  - 1 role string
  - 1 domain string ("ae")
  - n reviews: row index + `review_index` + delta timestamp

(Row indices refer to indices in the stored base+delta matrix, which is append-only. These indices are as good as identifiers).

The dominant factor governing the size of these files is the length of a hash, which is 16 bytes, as we discuss in the "Hashing" session below. Row indices and delta timestamps can be encoded in 4 bytes. Review indices take up 1 byte each. Estimating an upper bound of 1 million rows per dataset, a `.base` file would take around 32 MiB. A comprehensive `.review` file for such a dataset would take around 9 MiB.

These file structures are designed so that they start with a short heterogeneous header that reiterates the information that can be gleaned from the file name. The rest of the records are all homogeneous and of known size. That allows to load them into memory without the need for expensive parsing.

These files won't benefit much from compression since their main content (the hashes) is by construction statistically indistinguishable from noise.

#### Variable type encoding
The two "variable type" `.base` fields are encoded as single bytes that take the following values:

- Date: 1
- POSIXct: 2
- POSIXlt: 3
- Logical: 10
- Factor: 11
- Integer: 13
- Numeric: 14
- Complex: 15
- Character: 16
- Raw: 24

Most of these values are taken from the base R `SEXPTYPE` enum definition (see `src/include/Rinternals.h` on any recent R source distribution). 

The values assigned to time types are arbitrary, because they are S3 objects and thus lack dedicated `SEXPTYPE` values.

The type of a `factor()` variable is not fully defined by it being tagged as such, since the levels and their internal encoding is also part of the type. For purposes of hashing, the review feature of `dv.listings` treats the content of factor columns as `character()` by mapping their value to their assign string-like representation. This feature also is indifferent to a factor being ordered.

## Hashing
We store hashes for the values of `id_vars` and `tracked_vars` dataset columns. These hashes serve as content IDs.

#### Choice of hash function

A popular choice for non-cryptographic hashing is the `xxh` family of function (https://github.com/Cyan4973/xxHash/tree/dev?tab=readme-ov-file#xxhash---extremely-fast-hash-algorithm) for its balance of speed and perceived hash quality.
Three independent R packages implement them:

- rlang: Provides no control over serialization, so hashes are potentially not stable across R versions. 
- digest: Maintained by Dirk Eddelbuettel, known for being a stickler for backwards compatibility. Provides serialization options.
- xxhashlite: Substantially faster than the other two implementations for small input sizes (our scenario). Provides serialization options.

It makes sense to use the fastest implementation available, as it respects the time of users and requires less energy. `xxhashlite` is then the clear winner, but migration to `digest` (or "vendoring" of `xxhashlite` inside this package) is always possible if this lesser-known package is somehow retired from CRAN.

#### Hashing of ID variables (`hash_id()`)

With a perfect hash function, we would expect a collision after `2^(n/2)` entries have been hashed, where `n` is the output length in bits of the chosen hash. A hash that produces 64 bits is not strong enough as 2^32 is a large quantity of hashes, but still bound to collide if the review feature is used long enough.

We opt instead for 128 bits, which makes the possibility of a collision extremely unlikely. We expect an upper bound of one million rows per dataset, which is 13 *orders of magnitude* lower than the mean amount of entries estimated necessary to get a collision. The chosen hash won't be perfect, but we have an ample safety margin. We also assume that data has not been created through adversarial means geared towards making hash collisions more likely.

#### Hashing of tracked variables (`hash_tracked()`)
We could apply the same reasoning behind the choice of the `hash_id()` function to the hashing of the variable parts of each row. We instead propose a more complex hashing scheme to provide partial information about *which variables of a row have been altered* when its hash changes.

Each hash value is *m* bytes long, where *m* is the number of variables tracked of a given dataset. Each of those bytes is an independent hash of three of the tracked variables of a dataset row. Each variable, in turn, contributes to three of the *m* byte-sized hashes. This mixing of variables makes it harder for an external adversarial observer of the `.base` and `.delta` files to brute-force the original values of the dataset by looking for collisions with the computed hash values.

To compute which variables contribute to which hash byte, we use the following scheme:

  - Byte *n*: Variables (*n*+0)%*n*, (*n*+2)%*n* and (*n*+3)%*n*

Where `%` indicates the remainder of the integer division.

So, for a input dataset with seven tracked variables (zero through six), this would mean:

  - Byte 0: Variables 0, 2 and 3
  - Byte 1: Variables 1, 3 and 4
  - Byte 2: Variables 2, 4 and 5
  - Byte 3: Variables 3, 5 and 6
  - Byte 4: Variables 4, 6 and 0
  - Byte 5: Variables 5, 0 and 1
  - Byte 6: Variables 6, 1 and 2

This scheme creates a unique mixtures of variables. Take, for instance, variable 0. It is combined with variables 2 and 3 on the zeroth byte, with variables 4 and 6 on the fourth byte and with 1 and 5 for the fifth byte.

Each of these bytes is computed by:

- Taking the three values to hash.
- Serializing them to text and concatenating them using the non-ASCII byte separator `1D` (also known as "group separator").
- Computing the `xxh32` hash and returning its most significant byte.

Informal testing (refer to `tests/testthat/tests-hash_tracked.R` for more details) of this hashing scheme shows the following properties:

- It's capable of identifying up to four modified variables per row (after that, it's preferable to give up and notify the whole row as modified).
- It has a very low **false negative rate** (a variable is modified without it being notified as such) of one for every 16 million row updates.
- It has a low **false positive rate** (a variable that retains its value is notified as modified). This only happens when there are actual changes to a row.

False positives are not critical, as they ask reviewers to consider a larger set of variables when re-reviewing a row that has been altered.

False negatives *are* critical, since they allow changes to go unreported to reviewers. This is why our choice of hash favors false positives over false negatives.

This composite hash function is a compromise found through heuristic exploration. If we come up with a better option, we can version it through the "format version number" present in `.base` and `.delta` files. This means that neither app creator nor app users have to concern themselves about the underlying `hash_tracked` representation.
